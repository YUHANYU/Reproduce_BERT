# 阅读论文 *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*，复现BERT模型--Bidirectional Encoder Representation from Transformer.
