阅读论文 *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*，复现**BERT**模型。
已经可以跑通BERT模型的pre-training阶段，正在寻找数据进行预训练，下一步会做fine-tuning
